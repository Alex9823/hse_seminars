{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Fundamentals\n",
    "\n",
    "## Readings: \n",
    "1. Bishop. Pattern Recognition and Machine Learning. p38-48\n",
    "1. Shapire. Boosting: Foundation and Algorithms. p 23-43\n",
    "\n",
    "## Outline\n",
    "1. Task formulation\n",
    "1. Loss function\n",
    "1. Empirical Risk minimization\n",
    "1. No free lunch Theorem\n",
    "1. Generalization bounds\n",
    "1. Bias Variance decomposition\n",
    "1. Overfitting and Regularization\n",
    "1. Validation: Leave One Out, Cross-Validation, Hold Out\n",
    "1. Usual pipeline for model training\n",
    "1. Optimal Bayesian classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Task Formulation\n",
    "\n",
    "### Taxonomy\n",
    "1. Supervised vs Unsupervised vs Reinforcement Learning\n",
    "1. Unsupervised Learning: Clustering vs Dimentionality Reduction\n",
    "1. Clustering: Soft vs Hard\n",
    "1. Supervised Learning: Classification vs Regression\n",
    "1. Discriminative vs Generative Models\n",
    "1. Structured vs unstructured prediction\n",
    "1. ...\n",
    "\n",
    "Here we consider only **classification or regression tasks**.\n",
    "\n",
    "Given dataset $\\{ (x_i, y_i) \\}_{i=1}^N $ of i.i.d. objects\n",
    "\n",
    "Or, equvalently given:  \n",
    "$X \\in R^{Nxd}$ - feature matrix, where $d$ is dimension of feature space and $N$ - number of objects.  \n",
    "$Y \\in R^{N}$ - target vector\n",
    "\n",
    "For classification $Y \\in \\{0,1, ... C-1\\}^N$, where $C$ is a number of classes \n",
    "\n",
    "We want to find such algorithm $h \\in H$ that \"assigns for each object the right target value\".\n",
    "\n",
    "Classification\n",
    "<img src=\"images/classification.png\" style=\"height:300px\">\n",
    "\n",
    "Regression\n",
    "<img src=\"images/regression.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Loss function\n",
    "\n",
    "$Loss: R x R \\rightarrow R $ - loss function, that evaluates how bad our prediction for particular object are.\n",
    "\n",
    "Some loss functions:\n",
    "* $Loss(\\hat y, y) = (\\hat y - y)^2$\n",
    "* $Loss(\\hat y, y) = |\\hat y - y|$\n",
    "* $Loss(\\hat y, y) = \\frac {|\\hat y - y|} {y}$\n",
    "* $Loss(\\hat y, y) = I[\\hat y \\neq y]$\n",
    "\n",
    "\n",
    "For binary classification with $y \\in {-1,1}$ a variable $z = yh(x)$ is called **margin**. Positive margin corresponds to successful classification, negative margin corresponds to error. $|yh(x)|$ is a distance to decision boundary, which can be interpreted as confidence in classification of the object.     \n",
    "<img src=\"images/margin1.png\" style=\"height:300px\">\n",
    "\n",
    "Loss functions also can be determined in terms of margin:\n",
    "* Hinge loss   $Loss(\\hat y, y) = max(0, 1 - yf(x))$ \n",
    "* AdaBoost loss  $Loss(\\hat y, y) = e^{-yh(x)}$ \n",
    "* Logistic loss  $Loss(\\hat y, y) = \\log(1 + e^{-yh(x)})$ \n",
    "* Classification error  $Loss(\\hat y, y) = I[yh(x) < 0]$  \n",
    "\n",
    "<img src=\"images/margin_loss.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Empirical Risk Minimization\n",
    "\n",
    "In general, we want to optimize Expected Risk:  \n",
    "$$R = E [ Loss(x, y)] = \\int_{-\\infty}^{\\infty}Loss(x, y)dP(x,y) = Pr_{(x_i, y_i) \\sim D} [Loss(x_i, y_i)] $$\n",
    "\n",
    "But since we don't now the joint distibution $P(x,y)$, we can only deal with Empirical Risk (Loss functional):  \n",
    "$$\\hat R = \\sum_{i=1}^{N}Loss(x_i, y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Generalization bounds\n",
    "\n",
    "**Hoeffding’s inequality**:\n",
    "Let $x_1$,..., $x_m$ be independent random variables such that {$x_i \\in$ [0,1]}.\n",
    "Denote their average value by $A_m = \\frac 1 m \\sum_i^m x_i$\n",
    "Then for any $\\epsilon > 0$ we have \n",
    "\n",
    "$$ Pr[A_m > E[A_m] + \\epsilon] \\leq e^{-2m\\epsilon^2}$$\n",
    "\n",
    "Speaking about risk miminization, we can ask a question, how well does $\\hat R$ approximates $R$?\n",
    "\n",
    "Given m random examples, and for any $\\delta > 0$, we can deduce that with probability $Pr >= 1 - \\delta$, the following upper bound holds on the generalization error of $h$:\n",
    "$$R \\leq \\hat R + \\sqrt{ \\frac {\\ln(1 / \\delta)} {2m} }$$\n",
    "\n",
    "For finite hypothesis space $H$ under the same conditions:\n",
    "$$R \\leq \\hat R + \\sqrt{ \\frac {\\ln|H| + \\ln(1 / \\delta)} {2m} }$$\n",
    "\n",
    "\n",
    "\n",
    "**VC dimention** is defined as the cardinality of the largest set of points that the algorithm can shatter.\n",
    "\n",
    "<img src=\"images/vc_dim.png\" style=\"height:300px\">\n",
    "\n",
    "Let H be a hypothesis space of VC-dimension $d \\le \\infty$, and assume that arandom training set of size $m$ is chosen where $m \\geq d \\geq 1$. Then for any $\\epsilon > 0$,\n",
    "\n",
    "$$R \\leq \\hat R + \\sqrt{ \\frac {d\\ln(\\frac m d) + \\ln(\\frac 1 {\\delta})} {2m} }$$\n",
    "\n",
    "**Rademacher complexity**:\n",
    "Suppose now that the labels $y_i$ are chosen at random without regard to the $x_i$. In other words, suppose we replace each $y_i$ by a random variable $\\sigma_i$ that is −1 or +1 with\n",
    "equal probability, independent of everything else. Thus, the $\\sigma_i$ represent labels that are pure noise. We can measure how well the space $H$ can fit this noise in expectation\n",
    "by  \n",
    "$$E_{sigma} [\\max_{h \\in H} \\frac 1 m \\sum _{i=1}^m \\sigma_i h(x_i)]$$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 No free Lunch Theorem\n",
    "\n",
    "The No Free Lunch Theorems state that any one algorithm that searches for an optimal cost or fitness solution is not universally superior to any other algorithm.\n",
    "\n",
    "\"If an algorithm performs better than random search on some class of problems then in must perform worse than random search on the remaining problems.\" (No Free Lunch Theorems for Optimisation)\n",
    "\n",
    "How that affects machine learning?\n",
    "Every machine learning algorithm explicitly or implicitly implies some assumpsions made about observed data. So by contradicting these assumptions for every algorithm we create such dataset, where it achieves bad perfomance.\n",
    "\n",
    "<img src=\"images/lunch.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Bias variance decomposition\n",
    "\n",
    "Suppose our data is generated by:  \n",
    "$$y = f(x) + \\epsilon$$, where $\\epsilon \\in N(0,\\sigma)$ is white noise.  \n",
    "We want to build such estimator, that:  \n",
    "$$\\hat y = h(x)$$ is our prediction  \n",
    "\n",
    "Consider MSE regression  \n",
    "\n",
    "$$MSE = E[(y - h(x))^2] =\\\\\n",
    "E[(y - f(x) + f(x) - h(x))^2] =\\\\\n",
    "E[(y - f(x))^2] + E[(f(x) - h(x))^2] - 2E[(y - f(x)(f(x) - h(x))] =\\\\\n",
    "E[\\epsilon^2] + E[(f(x) - h(x))^2] - 2(E[yf(x)] - E[yh(x)] - E[f^2(x)] + E[f(x)h(x)] ) $$  \n",
    "\n",
    "Notes:  \n",
    "* since f is deterministic then $E[f^2(x)] = f^2(x)$   \n",
    "* since $E[y] = f(x)$ then $E[yf(x)] = f^2(x)$     \n",
    "* $E[yh(x)] = E[f(x)h(x)] + E[\\epsilon h(x)] = E[f(x)h(x)] + 0$  \n",
    "\n",
    "\n",
    "$$ MSE = E[\\epsilon^2] + E[(f(x) - h(x))^2] - 2(f^2(x) - E[f(x)h(x)] + 0 - f^2(x) + E[f(x)h(x)]) =\\\\  \n",
    "E[\\epsilon^2] + E[(f(x) - h(x))^2] =\\\\\n",
    "E[\\epsilon^2] + E[(f(x) - E[h(x)] + E[h(x)] -  h(x))^2] =\\\\\n",
    "E[\\epsilon^2] + E[(f(x) - E[h(x)])^2 ] + E[(E[h(x)] -  h(x))^2] + 2E[(E[h(x)] - h(x))(f(x) - E(h(x))] =\\\\  \n",
    "E[\\epsilon^2] + E[(f(x) - E[h(x)])^2 ] + E[(E[h(x)] -  h(x))^2] + 2(E[f(x)E[h(x)]] -E[E[h(x)]^2]  - E[h(x)f(x)] + E[h(x)E[h(x)]])$$  \n",
    "\n",
    "Notes:  \n",
    "* $E[fE[h(x)]] = f(x)E[h(x)]$    \n",
    "* $E[E[h(x)]^2] = E[h(x)]^2$  \n",
    "* $E[f(x)h(x)] = f(x)E[h(x)]$    \n",
    "* $E[h(x)E[h(x)]] = E[h(x)]^2$   \n",
    "\n",
    "$$ MSE = E[\\epsilon^2] + E[(f(x) - E[h(x)])^2 ] + E[(E[h(x)] -  h(x))^2] + 2(f(x)E[h(x)] -E[h(x)]^2  - f(x)E[h(x)] + E[h(x)]^2) =\\\\\n",
    "E[\\epsilon^2] + E[(f(x) - E[h(x)])^2 ] + E[(E[h(x)] -  h(x))^2] =\\\\\n",
    "Var[\\epsilon] + E[(f(x) - E[h(x)])^2 ] + Var[h(x)] =\\\\\n",
    "Var[\\epsilon] + bias^2 + Var[h(x)]$$   \n",
    "\n",
    "So prediction error can be decomposed into:\n",
    "1. variance of the noise   \n",
    "1. bias of prediction  \n",
    "1. variance of prediction\n",
    "\n",
    "<img src=\"images/decomp.jpeg\" style=\"height:300px\">\n",
    "\n",
    "### Validation curve\n",
    "\n",
    "Validation curve is a dependence of model perfomance on the model complexity\n",
    "\n",
    "Training and test error\n",
    "<img src=\"images/dec2.png\" style=\"height:300px\">\n",
    "\n",
    "Generalization error\n",
    "<img src=\"images/dec1.png\" style=\"height:300px\">\n",
    "\n",
    "### Learning curves\n",
    "\n",
    "Learning curve is a dependence of model perfomance on the size of training dataset\n",
    "\n",
    "High bias\n",
    "<img src=\"images/lc_bias.png\" style=\"height:300px\">\n",
    "\n",
    "High variance\n",
    "<img src=\"images/lc_var.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Overfitting and Regularization\n",
    "Overfitting is a situation, when a model fitted on a train dataset shows worse perfomance on a test dataset.  \n",
    "It corresponds to the fact, that model learns the given dataset but do not generalize to unseen data from the same distribution.  \n",
    "Every model does overfit!  \n",
    "\n",
    "<img src=\"images/overfitting.png\" style=\"height:200px\">\n",
    "\n",
    "\n",
    "In response to overfitting, there is regularization techniques.  \n",
    "In general, they correspond to setting restrictions onto hyposesis spaces, making generalization bound more tight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Validation\n",
    "\n",
    "Till now:\n",
    "\n",
    "1. $\\{x_i, y_i\\}_{i=1}^N$ is sampled from some joint distribution $P(x,y)$\n",
    "1. split into to non-overlapping subsets (train and test)  \n",
    "1. model h(x_i; w, \\theta) is described by its trainable weights $w$ and non-trainable hyperparams $\\theta$  \n",
    "1. choose some hyperparam value $\\theta=\\theta_0$ and train model $$\\sum_{i \\in train} Loss( h(x_i; w, \\theta_0), y_i) \\rightarrow \\min_{w}$$\n",
    "1. test model perfomance on test dataset $$\\hat R(\\theta_0) = \\sum_{i \\in test} Loss( h(x_i; w, \\theta_0), y_i)$$\n",
    "1. we expect that it is a good approximation of true generalization error $$R(\\theta_0) = E_{(x,y) \\sim P(x,y)} [ Loss( h(x_i; w, \\theta_0), y_i)]$$\n",
    "1. How to choose hyperparam $\\theta$?\n",
    "\n",
    "Usually want to optimize hyperparams by testing several values of $\\theta$ on the test set and choosing the best one.\n",
    "But the perfomance on the test set $\\hat R(\\theta)$(empirical risk) is a random variable, which can depend on the particular train\\test split! Here validation comes into play.\n",
    "\n",
    "We can say that $\\hat R(\\theta)$ is **point estimate** of expected risk $R(\\theta)$, which has its bias and variance.\n",
    "Different validation schemes try to minimize bias or variance or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Leave One Out\n",
    "Given dataset of m objects, create m experiments:  \n",
    "1. create split (m-1):1\n",
    "1. train on (m-1) object\n",
    "1. evaluate perfomance on the m-th object\n",
    "1. change split\n",
    "Average perfomance over all experiments\n",
    "\n",
    "Properties:\n",
    "1. Low variance of estimate\n",
    "2. High bias of estimate\n",
    "3. O(m) complexity\n",
    "4. Usually done when we have very small dataset\n",
    "5. There are perfomance metrics (e.g. AUC) that cannot be computed just on one sample.\n",
    "\n",
    "<img src=\"images/loo.png\" style=\"height:200px\">\n",
    "\n",
    "## 2 Hold Out\n",
    "Given dataset of m objects, create m experiments:  \n",
    "1. create split train:val, usually in proportion 80:20\n",
    "1. train on train subset\n",
    "1. evaluate perfomance on the val subset\n",
    "\n",
    "Properties:\n",
    "1. High variance of estimate\n",
    "1. Low bias of estimate\n",
    "1. O(1) complexity\n",
    "1. Usually done when when we have large dataset and\\or very heavy model\n",
    "\n",
    "<img src=\"images/holdout.png\" style=\"height:200px\">\n",
    "\n",
    "\n",
    "## 3 k-fold Cross Validation\n",
    "k = number of folds  \n",
    "folds = non-intersecting subsets of the dataset  \n",
    "Given dataset of m objects, create k experiments:  \n",
    "1. create split for k-1:1\n",
    "1. train on (k-1) folds\n",
    "1. evaluate perfomance on the k-th\n",
    "1. change split\n",
    "Average perfomance over all experiments\n",
    "\n",
    "Properties:\n",
    "1. Moderate variance of estimate\n",
    "2. Moderate bias of estimate\n",
    "3. O(k) complexity\n",
    "4. Usually done with k=5 or k=10\n",
    "\n",
    "<img src=\"images/cv.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Common model training pipeline\n",
    "\n",
    "1. Split dataset for train, test and validation parts\n",
    "1. train model on the train dataset without regularization, try to achive zero training loss\n",
    "1. add regularization, watch perfomance on the validation dataset\n",
    "1. test final model perfomance on test dataset. Choose between different model families.\n",
    "\n",
    "On the test dataset model must be evaluated by chosen quality metric, not by loss function used in model optimization.\n",
    "\n",
    "Regression\n",
    "<img src=\"images/pipeline.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Optimal Bayesian classifier\n",
    "\n",
    "Optimal Bayesian classifier is the best possible classifier given we know all joint distribution of features and labels. Which is unrealistic assumption.\n",
    "\n",
    "Suppose we have 2 classes.\n",
    "\n",
    "<img src=\"images/bayes.png\" style=\"height:400px\">\n",
    "\n",
    "Bayesian risk:\n",
    "$$ R = \\sum_{x,y} I[h(x) \\neq y] P(x,y)c_y$$, \n",
    "where $c_y$ is cost function for misclassification\n",
    "\n",
    "By applying Bayes rule:  \n",
    "$$P(y |X) = \\frac {P(X|y) P(y) c_y} {P(X)}$$\n",
    "\n",
    "$$h(x) = \\arg \\max_y P(X | y) P(y) c_y$$ is optimal Bayesian classifier\n",
    "\n",
    "**Decision function h(x)**\n",
    "We assign y = 1 iff:  \n",
    "$$ P(X | y=1) P(y=1)c_1 > P(X | y=0) P(y=0)c_0 $$  \n",
    "\n",
    "$$ \\frac {P(X | y=1)}{P(X | y=0)} > \\frac {P(y=0)c_0} {P(y=1)c_1} $$  \n",
    "\n",
    "$\\implies$Cost function and prior class probabilties are interchangable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
